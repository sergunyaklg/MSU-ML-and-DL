{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNtLJlW4v5VF"
      },
      "source": [
        "## Домашнее задание №8\n",
        "\n",
        "В данном задании вам предстоит детально рассмотреть механизм Attention (и реализовать несколько его вариантов), а также вернуться к задаче классификации текстов из задания №6 и решить ее с использованием BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "puOIr5xs4-_5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output \n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SfxedMT4-__"
      },
      "source": [
        "### Задание №2 (опциональное). Классификация текстов с использованием предобученной языковой модели.\n",
        "\n",
        "Вновь вернемся к набору данных SST-2. Разобьем выборку на train и test аналогично заданию №6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gar9LXdc4-__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e4bd3d-bdc7-4a87-80ee-67295c183516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-30 11:30:59--  https://github.com/MSUcourses/Data-Analysis-with-Python/raw/main/Deep%20Learning/holdout_texts08.npy\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MSUcourses/Data-Analysis-with-Python/main/Deep%20Learning/holdout_texts08.npy [following]\n",
            "--2023-04-30 11:30:59--  https://raw.githubusercontent.com/MSUcourses/Data-Analysis-with-Python/main/Deep%20Learning/holdout_texts08.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45416 (44K) [application/octet-stream]\n",
            "Saving to: ‘holdout_texts08.npy’\n",
            "\n",
            "holdout_texts08.npy 100%[===================>]  44.35K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-04-30 11:30:59 (4.10 MB/s) - ‘holdout_texts08.npy’ saved [45416/45416]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Не меняйте этот фрагмент кода! Здесь мы скачиваем holdout-выборку,\n",
        "# Т.е. это данные, на которых нам заранее неизвестна разметка. \n",
        "# Мы должны будем применить к этим данным построенную модель, и в качестве ответа предоставить сгенерированные предсказания.\n",
        "\n",
        "# __________start of block__________\n",
        "\n",
        "!wget https://github.com/MSUcourses/Data-Analysis-with-Python/raw/main/Deep%20Learning/holdout_texts08.npy -O holdout_texts08.npy\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Не меняйте этот фрагмент кода! Здесь мы преобразуем holdout-выборку в numpy.array\n",
        "\n",
        "# __________start of block__________\n",
        "\n",
        "texts_holdout = np.load('holdout_texts08.npy', allow_pickle=True)\n",
        "texts_holdout[:5]\n",
        "\n",
        "# __________end of block__________"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlSrPx951Lxn",
        "outputId": "efeeaccb-3108-4206-a49f-27afa0ef8903"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['gee , a second assassin shot kennedy ?',\n",
              "       \"from the big giant titles of the opening credits to elmer bernstein 's perfectly melodic score , haynes gets just about everything right\",\n",
              "       'the movie is well shot and very tragic , and one to ponder after the credits roll',\n",
              "       'the movie quickly drags on becoming boring and predictable',\n",
              "       \"a mixed bag of a comedy that ca n't really be described as out of this world\"],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Pmkfbg0p4-__"
      },
      "outputs": [],
      "source": [
        "# Не меняйте код ниже! Здесь мы загружаем данные с известной разметкой. На них мы можем обучать модель и вычислять метрики качества классификации.\n",
        "# __________start of block__________\n",
        "df = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/MSUcourses/Data-Analysis-with-Python/main/Deep%20Learning/Files/STT2_train_task08.tsv',\n",
        "    delimiter='\\t',\n",
        "    header=None\n",
        ")\n",
        "texts_train = df[0].values[:5000]\n",
        "y_train = df[1].values[:5000]\n",
        "texts_test = df[0].values[5000:]\n",
        "y_test = df[1].values[5000:]\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu73hCSQ4-__"
      },
      "source": [
        "Весь остальной код предстоит написать вам.\n",
        "\n",
        "Для успешной сдачи на максимальный балл необходимо добиться хотя бы __84.5% accuracy на тестовой части выборки__."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Устанавливаем библиотеку transformers\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Токенизируем тексты\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_texts(texts, max_len=512):\n",
        "    input_ids, attention_masks = [], []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "input_ids_train, attention_masks_train = tokenize_texts(texts_train)\n",
        "input_ids_test, attention_masks_test = tokenize_texts(texts_test)\n",
        "input_ids_holdout, attention_masks_holdout = tokenize_texts(texts_holdout)\n",
        "\n",
        "# Создаем датасеты и загрузчики данных\n",
        "batch_size = 8\n",
        "\n",
        "train_data = TensorDataset(input_ids_train, attention_masks_train, torch.tensor(y_train))\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(input_ids_test, attention_masks_test, torch.tensor(y_test))\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "holdout_data = TensorDataset(input_ids_holdout, attention_masks_holdout)\n",
        "holdout_sampler = SequentialSampler(holdout_data)\n",
        "holdout_dataloader = DataLoader(holdout_data, sampler=holdout_sampler, batch_size=batch_size)\n",
        "\n",
        "# Обучаем модель\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Переносим модель на устройство GPU, если доступно\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Инициализация оптимизатора и планировщика\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 3\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_masks = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss, logits = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)[:2]\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    return avg_train_loss\n",
        "\n",
        "# Обучение модели\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_epoch()\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n",
        "\n",
        "# Оценка модели\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    probas = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_masks = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids, token_type_ids=None, attention_mask=attention_masks).logits\n",
        "\n",
        "        probas.extend(F.softmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "    return np.array(probas)\n",
        "\n",
        "# Получение предсказаний на тренировочной, тестовой и holdout выборках\n",
        "train_probas = evaluate(model, train_dataloader)\n",
        "test_probas = evaluate(model, test_dataloader)\n",
        "holdout_probas = evaluate(model, holdout_dataloader)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, np.argmax(train_probas, axis=1))\n",
        "test_accuracy = accuracy_score(y_test, np.argmax(test_probas, axis=1))\n",
        "\n",
        "print(f\"Train accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "024vQ3Cu6yKt",
        "outputId": "55d7e6f1-d441-4613-8f9f-853cfc8b3c67"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 0.06618559886608273\n",
            "Train accuracy: 0.5016\n",
            "Test accuracy: 0.9125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqZjx0pF4-__"
      },
      "source": [
        "#### Сдача задания в контест\n",
        "Сохраните в словарь `out_dict` вероятности принадлежности к нулевому и первому классу соответственно:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YBmeirTH4-__"
      },
      "outputs": [],
      "source": [
        "out_dict = {\n",
        "    'train': train_probas,\n",
        "    'test': test_probas,\n",
        "    'holdout': holdout_probas,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk2gndCN4-__"
      },
      "source": [
        "Несколько `assert`'ов для проверки вашей посылки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XGUi9p_14-__"
      },
      "outputs": [],
      "source": [
        "assert isinstance(out_dict['train'], np.ndarray), 'Dict values should be numpy arrays'\n",
        "assert out_dict['train'].shape == (5000, 2), 'The predicted probas shape does not match the train set size'\n",
        "assert np.allclose(out_dict['train'].sum(axis=1), 1.), 'Probas do not sum up to 1 for some of the objects'\n",
        "\n",
        "assert isinstance(out_dict['test'], np.ndarray), 'Dict values should be numpy arrays'\n",
        "assert out_dict['test'].shape == (1920, 2), 'The predicted probas shape does not match the test set size'\n",
        "assert np.allclose(out_dict['test'].sum(axis=1), 1.), 'Probas do not sum up to 1 for some of the object'\n",
        "\n",
        "assert isinstance(out_dict['holdout'], np.ndarray), 'Dict values should be numpy arrays'\n",
        "assert out_dict['holdout'].shape == (500, 2), 'The predicted probas shape does not match the holdout set size'\n",
        "assert np.allclose(out_dict['holdout'].sum(axis=1), 1.), 'Probas do not sum up to 1 for some of the object'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ann49yp64-__"
      },
      "source": [
        "Запустите код ниже для генерации посылки и сдайте файл `submission_dict_hw08.npy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TQTs8S2p4_AA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab4698f-57e9-42e7-af4c-de5e6f7f3750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to `submission_dict_hw08.npy`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "np.save('submission_dict_hw08.npy', out_dict, allow_pickle=True)\n",
        "print('File saved to `submission_dict_hw08.npy`')\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hIF-ntR4_AA"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}